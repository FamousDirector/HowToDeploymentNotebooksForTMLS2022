{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e313a71-45a8-4f1b-96a9-42eb73361d3e",
   "metadata": {},
   "source": [
    "# Model Frameworks and Inference Latency\n",
    "\n",
    "Not all frameworks for ML models are cut from the same cloth. While frameworks like PyTorch or TensorFlow are designed to train and infer models, frameworks like ONNX or TensorRT are designed with inference in mind.\n",
    "\n",
    "It can be more difficult (or impossible without custom C++ kernels) to get a model into an optimized format. In this notebook some common formats will be explored and the conversion process and benefits will be demonstrated.\n",
    "\n",
    "*(Note this section will take a fair amount of device memory. If you run out of memory remember to close/stop other Jupyter Notebook kernels and if necessary only do a few models at a time!)* There are portions of this notebook that will stop the kernal to regain some device memory to try and be proactive in this regard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eaf5f8-276e-4ce4-8ebd-b7beb04e8d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# create data input\n",
    "image_file_path = \"sample_images/group-photo.jpg\"\n",
    "\n",
    "target_input_height = 480\n",
    "target_input_width = 640\n",
    "\n",
    "os.environ['TARGET_HEIGHT'] = str(target_input_height)\n",
    "os.environ['TARGET_WIDTH'] = str(target_input_width)\n",
    "\n",
    "original_image = cv2.imread(image_file_path)\n",
    "\n",
    "# pre processing\n",
    "def pre_process_image(original_image):\n",
    "    resized_image = cv2.resize(original_image, (target_input_width,\n",
    "                                 target_input_height))\n",
    "    image_rgb = cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB)\n",
    "    image = np.float32(image_rgb)\n",
    "\n",
    "    image = image/255\n",
    "    image = np.moveaxis(image, -1, 0)  # HWC to CHW\n",
    "\n",
    "    image = image[np.newaxis, :] # add batch dimension\n",
    "    image = np.float32(image)\n",
    "    \n",
    "    return image\n",
    "\n",
    "image = pre_process_image(original_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb5ba70-ba41-4b24-abe6-1cc992d2b2ca",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "\n",
    "Pytorch is a great framework for training models and can be used for inference as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f214cc4f-a2e3-415b-bf1e-f72800f7e6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gdown\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'DM-Count/'))\n",
    "\n",
    "import models\n",
    "\n",
    "model_path = \"model.pth\"\n",
    "url = \"https://drive.google.com/uc?id=1nnIHPaV9RGqK8JHL645zmRvkNrahD9ru\"\n",
    "gdown.download(url, model_path, quiet=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# load model\n",
    "model = models.vgg19() # DM-Count is VGG19 based\n",
    "model.load_state_dict(torch.load(model_path, device))\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d7bfc0-bce1-4046-8981-bb99e2393357",
   "metadata": {},
   "source": [
    "## Torchscript\n",
    "\n",
    "[TorchScript](https://pytorch.org/docs/stable/jit.html) is a way to create serializable and optimizable models from PyTorch code. Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency.\n",
    "\n",
    "Typically, Pytorch models are converted to Torchscript for deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7efd41e-fdc5-4ae8-b44c-f64d0c83e4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.rand(1, 3, target_input_height, target_input_width)\n",
    "\n",
    "traced_model = torch.jit.trace(model, dummy_input).eval()\n",
    "scripted_model = torch.jit.script(traced_model).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a206be5f-e057-4553-afe5-12e00d0a4988",
   "metadata": {},
   "source": [
    "## ONNX\n",
    "\n",
    "ONNX is an open format built to represent machine learning models. Read more about it [here](https://onnx.ai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48610990-b979-4cb7-942d-9219fea2e641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model in ONNX format\n",
    "dummy_input = torch.rand(1, 3, target_input_height, target_input_width)\n",
    "\n",
    "torch.onnx.export(model,  # model being run\n",
    "                  dummy_input,  # model test input\n",
    "                  \"model.onnx\",  # where to save the model (can be a file or file-like object)\n",
    "                  opset_version=16,  # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names=['input'],  # the model's input names\n",
    "                  output_names=['output_0', 'output_1'],  # the model's output names\n",
    "                  dynamic_axes={'input': {0: 'batch_size'},  # variable length axes\n",
    "                                'output_0': {0: 'batch_size'}, \n",
    "                                'output_1': {0: 'batch_size'}\n",
    "                               }\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e108de-ebec-48b7-adb3-12ee47e985d2",
   "metadata": {},
   "source": [
    "Polygrapy is an excellent toolkit designed to assist in running and debugging deep learning models in various frameworks. It is preinstalled with all containers from Nvidia's [NGC](https://catalog.ngc.nvidia.com/).\n",
    "\n",
    "Here we are going to use it to \"sanitize\" the model, by folding constants in the model graph into other nodes. This can simplyfiy the network and potentially give some speedup. Another tool that can do this is [ONNX Simplifier](https://github.com/daquexian/onnx-simplifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835dc3c3-3371-4446-918a-0e4f36b200c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!POLYGRAPHY_AUTOINSTALL_DEPS=1 polygraphy surgeon sanitize model.onnx --fold-constants -o model_simplified_folded.onnx "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15908e8b-e8c0-4a72-a612-481e2e97fb26",
   "metadata": {},
   "source": [
    "## TensorRT\n",
    "\n",
    "NVIDIA® TensorRT™, is an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applications on NVIDIA hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5629cfea-5251-4f88-85c9-fbedc0ab9137",
   "metadata": {},
   "outputs": [],
   "source": [
    "!POLYGRAPHY_AUTOINSTALL_DEPS=1 polygraphy convert model_simplified_folded.onnx --convert-to trt --output model.engine --trt-min-shapes input:[1,3,$TARGET_HEIGHT,$TARGET_WIDTH] --trt-opt-shapes input:[1,3,$TARGET_HEIGHT,$TARGET_WIDTH] --trt-max-shapes input:[8,3,$TARGET_HEIGHT,$TARGET_WIDTH]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee44c53-d390-4c16-8704-cbd45af36248",
   "metadata": {},
   "source": [
    "## Measuring the latency difference\n",
    "Now that all the models have been converted to the frameworks, lets do some simple benchmarking.\n",
    "\n",
    "Note: the `min()` is taken to avoid the warmup values and any spikes that could be caused from other processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115cdc59-ffe4-45db-9252-408780e9a6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "raw_torch_latency = []\n",
    "torchscript_latency = []\n",
    "onnx_latency = []\n",
    "trt_latency = []\n",
    "\n",
    "number_of_iterations = 25\n",
    "batch_size_range = [1, 2, 4, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263e940c-ec03-43f6-9300-de106daf9162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send Pytorch model to compute device\n",
    "model = model.to(device)\n",
    "\n",
    "def raw_torch_infer(image):\n",
    "    if not torch.is_tensor(image):\n",
    "        image = torch.tensor(image)\n",
    "    image_tensor = image.to(device) # move image data to compute device    \n",
    "    output = model(image_tensor)[0].cpu() # inference    \n",
    "    image.cpu()\n",
    "    del image\n",
    "    return output   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9724ad7c-dd22-4bcc-a888-1bf7abb8b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in batch_size_range:\n",
    "    image_batch = np.concatenate([image for _ in range(n)])\n",
    "    raw_torch_latency.append(min(timeit.repeat(lambda: raw_torch_infer(image_batch), number=number_of_iterations)) / number_of_iterations / n)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bab4535-4c90-4596-841d-eb11aad6c060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unload model from GPU to save some memory\n",
    "model.cpu()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27a6dfb-2ed3-4fc6-9726-026ecdeef0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_model = scripted_model.to(device)\n",
    "\n",
    "def torchscript_infer(image):\n",
    "    if not torch.is_tensor(image):\n",
    "        image = torch.tensor(image)\n",
    "    image_tensor = image.to(device) # move image data to compute device    \n",
    "    output = scripted_model(image_tensor)[0].cpu() # inference    \n",
    "    image.cpu()   \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34fcc6e-04e6-4eaa-916c-6c92b07ad1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in batch_size_range:\n",
    "    image_batch = np.concatenate([image for _ in range(n)])\n",
    "    torchscript_latency.append(min(timeit.repeat(lambda: torchscript_infer(image_batch), number=number_of_iterations)) / number_of_iterations / n)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e573d96-a06f-48d0-ae6b-6d4f7d0c8045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unload model from GPU to save some memory\n",
    "scripted_model.cpu()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a959ff-0bf2-4c8d-b81e-fc0d52bbb100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "# instatiate model\n",
    "ort_sess = ort.InferenceSession(f'model_simplified_folded.onnx', providers=['CUDAExecutionProvider'])\n",
    "\n",
    "def onnx_infer(image):\n",
    "    output = ort_sess.run(None, {\"input\": image})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cf4331-4c02-4757-9a5a-a05994c3e134",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in batch_size_range:\n",
    "    image_batch = np.concatenate([image for _ in range(n)])\n",
    "    onnx_latency.append(min(timeit.repeat(lambda: onnx_infer(image_batch), number=number_of_iterations)) / number_of_iterations / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b35f7c-94ce-4698-ab2b-db2a6be0d798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unload model from GPU to save some memory\n",
    "del ort_sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9f22ab-0804-4335-99ba-efac624b692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from polygraphy.backend.common import BytesFromPath\n",
    "from polygraphy.backend.trt import EngineFromBytes, TrtRunner\n",
    "\n",
    "# instatiate model\n",
    "load_engine = EngineFromBytes(BytesFromPath(f\"model.engine\"))\n",
    "trt_runner = TrtRunner(load_engine)\n",
    "trt_runner.activate()\n",
    "\n",
    "def trt_infer(image):\n",
    "    output = trt_runner.infer(feed_dict={\"input\": image})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f197b9b8-8bee-4a2c-8a67-34e16fa1e9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in batch_size_range:\n",
    "    image_batch = np.concatenate([image for _ in range(n)])\n",
    "    trt_latency.append(min(timeit.repeat(lambda: trt_infer(image_batch), number=number_of_iterations)) / number_of_iterations / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548ba6af-461e-474f-97f2-65b12a675b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unload model from GPU to save some memory\n",
    "trt_runner.deactivate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fbf534-fd4a-4115-9c11-c89cdd10818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "objects = ('Raw PyTorch', 'TorchScript', 'ONNX', 'TensorRT')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [raw_torch_latency,torchscript_latency,onnx_latency,trt_latency]\n",
    "\n",
    "for p, o in zip(performance, objects):  \n",
    "    plt.plot(batch_size_range, p, label=o)\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Latency (s)')\n",
    "plt.title('Latency of DM-Count')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe9df64-c9f0-4776-a3b2-7a95c0222207",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Look! Using a framework like ONNX or TensorRT can lower your inference speed quite significantly. Also if you look at the memory consumed as models are loaded you can see differences between the frameworks as well.\n",
    "\n",
    "Batching can be very useful to increase throughput as show in this graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189850c4-ad98-40b7-bb56-d0861596c0fc",
   "metadata": {},
   "source": [
    "## Quantization \n",
    "\n",
    "\"Quantization refers to techniques for performing computations and storing tensors at lower bitwidths than floating point precision. A quantized model executes some or all of the operations on tensors with reduced precision rather than full precision (floating point) values. This allows for a more compact model representation and the use of high performance vectorized operations on many hardware platforms.\" Here is a good [site](https://pytorch.org/docs/stable/quantization.html) to start learning more.\n",
    "\n",
    "Lets start by quantizing models in TensorRT and Torchscript!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5df77f2-6163-469f-ba49-5dbbc0a81c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "!POLYGRAPHY_AUTOINSTALL_DEPS=1 polygraphy convert model_simplified_folded.onnx --convert-to trt --output model_fp16.engine --fp16 --trt-min-shapes input:[1,3,$TARGET_HEIGHT,$TARGET_WIDTH] --trt-opt-shapes input:[1,3,$TARGET_HEIGHT,$TARGET_WIDTH] --trt-max-shapes input:[8,3,$TARGET_HEIGHT,$TARGET_WIDTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4051a032-cd47-4dd8-8685-5c7907cb50cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instatiate model\n",
    "load_engine = EngineFromBytes(BytesFromPath(f\"model_fp16.engine\"))\n",
    "trt_runner_fp16 = TrtRunner(load_engine)\n",
    "trt_runner_fp16.activate()\n",
    "\n",
    "def trt_infer_fp16(image):\n",
    "    output = trt_runner_fp16.infer(feed_dict={\"input\": image})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d697d246-06d9-4036-a62a-53be667f4f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark\n",
    "\n",
    "trt_fp16_latency = []\n",
    "\n",
    "for n in batch_size_range:\n",
    "    image_batch = np.concatenate([image for _ in range(n)])\n",
    "    trt_fp16_latency.append(min(timeit.repeat(lambda: trt_infer_fp16(image_batch), number=number_of_iterations)) / number_of_iterations / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d48f4c8-f99f-43a8-a479-2688c1c25d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unload model from GPU to save some memory\n",
    "trt_runner_fp16.deactivate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2017e648-fbb8-4509-aa58-4662a734e886",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_model_fp16 = scripted_model.half().to(device)\n",
    "\n",
    "def torchscript_infer_fp16(image):\n",
    "    if not torch.is_tensor(image):\n",
    "        image = torch.tensor(image)\n",
    "    image_tensor = image.to(device) # move image data to compute device    \n",
    "    output = scripted_model_fp16(image_tensor.half())[0].cpu() # inference\n",
    "    image.cpu()\n",
    "    del image\n",
    "    torch.cuda.empty_cache()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64404eb-0a85-43f6-8bc7-de778a281607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark\n",
    "\n",
    "torchscript_fp16_latency = []\n",
    "\n",
    "for n in batch_size_range:\n",
    "    image_batch = np.concatenate([image for _ in range(n)])\n",
    "    torchscript_fp16_latency.append(min(timeit.repeat(lambda: torchscript_infer_fp16(image_batch), number=number_of_iterations)) / number_of_iterations / n)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb20897d-f66d-491d-b4fa-e2452a3e24b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unload model from GPU to save some memory\n",
    "scripted_model_fp16.cpu()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d07c67-5cce-4469-9be6-fa6c2cddedba",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = ('Torchscript FP32', 'Torchscript FP16', 'TensorRT FP32', 'TensorRT FP16')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [torchscript_latency, torchscript_fp16_latency, trt_latency, trt_fp16_latency]\n",
    "\n",
    "for p, o in zip(performance, objects):  \n",
    "    plt.plot(batch_size_range, p, label=o)\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(True)\n",
    "plt.ylabel('Latency (s)')\n",
    "plt.title('Quantization Lowers Latency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cead7f7d-93b0-4c47-9c11-46470ed03675",
   "metadata": {},
   "source": [
    "Quantization gives a significant performance improvement to inference latency as well!\n",
    "\n",
    "Just ensure that when quantizing that the accuracy remains within your target. It would be expected to see ~0.1% accuracy drop for a FP16 bit quantization in a computer vision task for example.\n",
    "\n",
    "INT8, FP8 and other numerical representations can be used as well, but quantization can be more challenging. Here is a good [site](https://pytorch.org/docs/stable/quantization.html) to start learning more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dc8b0d-e170-4c4c-be4a-aff58550cd72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
