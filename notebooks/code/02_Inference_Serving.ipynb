{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cce439a7-f830-4c83-8733-1caa72d77c70",
   "metadata": {},
   "source": [
    "# Model Serving\n",
    "\n",
    "Centrailized model serving can be a huge design win for your buisness products and/or applications. Hosting models in a central location reduces memory usage and can be designed to reduce interdevice communication.\n",
    "\n",
    "This example will use [Nvidia's Triton Inference Server](https://github.com/triton-inference-server/server) to serve the model show in the previous section.\n",
    "\n",
    "First let's export the model to a serving format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892cecbb-39ce-469b-8745-27b923d525a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/cvlab-stonybrook/DM-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cd1122-385c-428c-9c36-58b33e7b8694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "import onnx\n",
    "import gdown\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'DM-Count/'))\n",
    "\n",
    "import models\n",
    "\n",
    "model_path = \"model.pth\"\n",
    "url = \"https://drive.google.com/uc?id=1nnIHPaV9RGqK8JHL645zmRvkNrahD9ru\"\n",
    "gdown.download(url, model_path, quiet=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# load model\n",
    "model = models.vgg19() # DM-Count is VGG19 based\n",
    "model.load_state_dict(torch.load(model_path, device))\n",
    "model.eval()\n",
    "\n",
    "# send model to compute device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b832075-a40d-4d8b-8385-16d4c033a4d9",
   "metadata": {},
   "source": [
    "We will now export our trained model for deployment. I am choosing the ONNX format for deployment. ONNX is an open format built to represent machine learning models. Read more about it [here](https://onnx.ai/). Triton can handle many different model formats and even can be used to serve custom Python scripts.\n",
    "\n",
    "Model formats for serving will be covered in a later section in more detail!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb78f7d-7572-4e54-9475-12afeb6d454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_input_width = 1280\n",
    "target_input_height = 800\n",
    "\n",
    "dummy_input = torch.rand(1, 3, target_input_height, target_input_width).to(device)\n",
    "\n",
    "torch.onnx.export(model,  # model being run\n",
    "                  dummy_input,  # model test input\n",
    "                  \"model.onnx\",  # where to save the model (can be a file or file-like object)\n",
    "                  opset_version=16,  # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names=['input'],  # the model's input names\n",
    "                  output_names=['output_0', 'output_1'],  # the model's output names\n",
    "                  dynamic_axes={'input': {0: 'batch_size'},  # variable length axes\n",
    "                                'output_0': {0: 'batch_size'}, \n",
    "                                'output_1': {0: 'batch_size'}\n",
    "                               }\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1233f157-9960-4c71-b79c-f4beb58433e9",
   "metadata": {},
   "source": [
    "One nice feature of Triton is the [ability to have it \"poll\" a model repository](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_management.md#model-control-mode-poll) to see if a change has occured. So all that needs to be done is copy the model into the `model_repository` directory. You can read more on the specifics [here](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_repository.md#repository-layout).\n",
    "\n",
    "Triton has been setup and running already within the `docker-compose` network created when starting this all up. It's host name is `triton-inference-server`.\n",
    "\n",
    "Okay let's get this model into Triton!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2f4655-7876-4098-a50a-139174fa4280",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir model_repository/dmcount_onnx/ # create the folder with the model name\n",
    "! mkdir model_repository/dmcount_onnx/1/ # create the folder for the model version\n",
    "!cp model.onnx model_repository/dmcount_onnx/1/ # move the file to the directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122217e5-bb96-4a76-a08d-db7f55176bd0",
   "metadata": {},
   "source": [
    "Now that the model is in Triton, it has automatically created a model config and loaded it. Let's query it to see more about it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798f1f58-f5db-428d-a337-629489e0584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.grpc as grpcclient # can use http or grpc\n",
    "\n",
    "# create the client\n",
    "inference_server_url = \"triton-inference-server:8001\"\n",
    "triton_client = grpcclient.InferenceServerClient(url=inference_server_url)\n",
    "\n",
    "# find out info about model\n",
    "model_name = \"dmcount_onnx\"\n",
    "triton_client.get_model_config(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14582bcf-cc06-4b59-9344-c3aab22849d2",
   "metadata": {},
   "source": [
    "You can also create a custom config to control other paramenters like batch size or maximum number of requests. See [here](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md) for more.\n",
    "\n",
    "However now we are going to do our inference with the model!\n",
    "\n",
    "You can see int he config above we have the input and output names of the model. Let's use this information to build a function for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9211107-e12e-4cb3-8c0a-3713d6f7b8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(image):\n",
    "    # create input\n",
    "    input_name = \"input\"\n",
    "    inputs = [grpcclient.InferInput(input_name, image.shape, \"FP32\")]\n",
    "    inputs[0].set_data_from_numpy(image)\n",
    "\n",
    "    output_names = [\"output_0\", \"output_1\"]\n",
    "    outputs = [grpcclient.InferRequestedOutput(n) for n in output_names]\n",
    "\n",
    "    results = triton_client.infer(model_name, inputs, outputs=outputs) # send the query\n",
    "\n",
    "    output_0, output_1 = [results.as_numpy(o) for o in output_names]\n",
    "    \n",
    "    return output_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a439a37-bb6d-406f-9c14-40bc528be29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tritonclient.utils import triton_to_np_dtype\n",
    "\n",
    "def detect_crowd(original_image):\n",
    "    '''\n",
    "    Function for counting crowds in a single image\n",
    "    '''\n",
    "        \n",
    "    resized_image = cv2.resize(original_image, (target_input_width, target_input_height))\n",
    "    \n",
    "    # preprocessing    \n",
    "    image_rgb = resized_image[...,::-1] # BGR to RGB\n",
    "    image = image_rgb.astype(np.float32)\n",
    "\n",
    "    image = image/255\n",
    "    image = np.transpose(image, (2, 0, 1))  # HWC to CHW\n",
    "\n",
    "    image = np.expand_dims(image, axis=0) # add batch dimension\n",
    "\n",
    "    # inference\n",
    "    output = infer(image)\n",
    "    \n",
    "       # post processing\n",
    "    crowd_count = int(np.sum(output).item())\n",
    "    \n",
    "    heatmap = output[0, 0]\n",
    "    heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-5)\n",
    "    heatmap = (heatmap * 255).astype(np.uint8)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "    heatmap = cv2.resize(heatmap, (target_input_width, target_input_height))\n",
    "        \n",
    "    overlayed = cv2.addWeighted(resized_image, 0.7, heatmap, 0.5, 0)\n",
    "    \n",
    "    return overlayed, crowd_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9a1478-5fff-4103-ada8-a2897fad6888",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We will now use the same Gradio interface as from the first section with the new way of model serving!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c99cef-c57c-4068-b31a-2a44535ac0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "gr.close_all() # cleanup any stray samples\n",
    "\n",
    "iface = gr.Interface(fn=detect_crowd,\n",
    "                     inputs=[\n",
    "                         gr.Image(label=\"Image of Crowd\"),\n",
    "                     ],\n",
    "                     outputs=[\n",
    "                         gr.Image(label=\"Predicted Density Map\"),\n",
    "                         gr.Label(label=\"Predicted Count\"),\n",
    "                     ],\n",
    "                     examples=[\n",
    "                         [\"sample_images/busy-road.jpg\"],\n",
    "                         [\"sample_images/concert-crowd.jpg\"],\n",
    "                         [\"sample_images/group-photo.jpg\"],\n",
    "                         [\"sample_images/mountains.jpg\"],\n",
    "                     ],\n",
    "                     title=\"Crowd Detection App\",\n",
    "                     description=\"A simple app.\",\n",
    "                     )\n",
    "iface.launch(server_name=\"0.0.0.0\", server_port=7860)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7752d231-11b0-4fb4-bbc8-c509cfca7670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
