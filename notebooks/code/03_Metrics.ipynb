{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d96b196-f0ca-4022-98e6-75ff98da02cd",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "Monitoring performance of your application once deployed is definitely a best pracitce and extrememly common regardless if AI is involved or not. Some popular tools for this are [Promethus](https://prometheus.io/) and [Grafana](https://grafana.com/) which we will be using in this example.\n",
    "\n",
    "If you have ran the previous notebooks some dashboards should already be populated with stats from Triton Inference Server and your NVIDIA GPU. Look here to see your Grafana dashboard for [Inference Server stats](http://localhost:3000/d/slEY4dsZk/triton-inference-server?orgId=1&refresh=5s) and here for [GPU stats](http://localhost:3000/d/Oxed_c6Wz/nvidia-dcgm-exporter-dashboard?orgId=1&refresh=5s)\n",
    "\n",
    "You can also add new stats by using a Promethus client (e.g. [Python](https://github.com/prometheus/client_python)) and add a new source by modifying `prometheus/config/prometheus.yml` file.\n",
    "\n",
    "There is an example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5abc59f-4fab-4dda-a0da-b101dea950a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prometheus_client import start_http_server, Summary\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Create a metric to track time spent and requests made.\n",
    "DETECTION_CONFIDENCE = Summary('detection_confidence', 'Detection confidence from the model', ['class_name']) \n",
    "# NOTE: there are different types of metrics like \"Summary\", \"Gauge\" and store data differently. Refer to the docs for more info!\n",
    "\n",
    "start_http_server(8000)\n",
    "\n",
    "def report_confidence(confidence_value, class_name):\n",
    "    DETECTION_CONFIDENCE.labels(class_name).observe(confidence_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4e12bf-a9a7-4706-858d-46285683c975",
   "metadata": {},
   "source": [
    "You can now report a custom metric and have it show up as a stat! (Note: it may take a few moments for the data to appear in Grafana!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b1421b-8691-458c-8f4f-535cd04fc514",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_confidence(0.95, \"test object\")\n",
    "report_confidence(0.65, \"new object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d6a393-826a-4147-8899-b435d93a9ab2",
   "metadata": {},
   "source": [
    "Reporting metrics, like model confidence, can be extremely important for both monitoring how the model is performing, but can also be used to inform MLOps pipelines and systems. For example, if the model has a series of low confidence detections you can capture and send this data to the Data Science team for analysis, or automatically add it to training data for additional robustness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
