{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c185717f-3cee-44a0-84d3-e83d8fb951ee",
   "metadata": {},
   "source": [
    "# Reduce Data Movement\n",
    "Larger pieces of data can take a non-insignificant amount of time to move. For instance a large image in a raw pixel format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f74f8b4-a5f1-4505-8d57-0b201d436024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "fake_image_data = torch.rand(1, 3, 800, 1280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114cf611-9275-4e07-95f2-ed3957f9f2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit \n",
    "fake_image_data.to('cuda')\n",
    "fake_image_data.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6768057a-02c7-4ccc-a7ef-b9e06f9f61be",
   "metadata": {},
   "source": [
    "Hosting the pre and post processing within your inference server is an easy way to prevent unecessary calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528b52a9-3b35-4a12-8bdc-5dd2d341d52c",
   "metadata": {},
   "source": [
    "# Compile pre/post processing\n",
    "Pre and post processing are often significant pieces of processing that should be accelerated to reduce end to end latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8b1397-cd52-4451-a914-e0bdf9145a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# create data input\n",
    "image_file_path = \"sample_images/group-photo.jpg\"\n",
    "\n",
    "target_input_height = 800\n",
    "target_input_width = 1280\n",
    "\n",
    "original_image = cv2.imread(image_file_path)\n",
    "\n",
    "\n",
    "resized_image = cv2.resize(original_image, (target_input_width,\n",
    "                           target_input_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647fde25-b2b3-42f1-9231-2c90423795fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre processing\n",
    "def pre_process_image(input_image):\n",
    "    image_rgb = input_image[...,::-1] # BGR to RGB\n",
    "    image = image_rgb.astype(np.float32)\n",
    "\n",
    "    image = np.divide(image, 255)\n",
    "    image = np.transpose(image, (2, 0, 1))  # HWC to CHW\n",
    "\n",
    "    image = np.expand_dims(image, axis=0) # add batch dimension\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176c3477-2a04-47aa-a9fa-eca1ec43d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit \n",
    "image = pre_process_image(resized_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48872393-c441-427e-b034-db9e3544733a",
   "metadata": {},
   "source": [
    "### Numba\n",
    "See here for more info: https://numba.pydata.org/numba-doc/latest/user/5minguide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab80a2bb-da7c-4894-922c-5e5518d02db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def fast_pre_process_image(input_image):\n",
    "    image_rgb = input_image[...,::-1] # BGR to RGB\n",
    "    image = image_rgb.astype(np.float32)\n",
    "\n",
    "    image = np.divide(image, 255)\n",
    "    image = np.transpose(image, (2, 0, 1))  # HWC to CHW\n",
    "\n",
    "    image = np.expand_dims(image, axis=0) # add batch dimension\n",
    "    \n",
    "    return image\n",
    "\n",
    "# note: it compiles the function first time it is used\n",
    "image = fast_pre_process_image(resized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a6b9a6-1b98-45a8-b4cb-669b3bde862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit \n",
    "image = fast_pre_process_image(resized_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f5c8a8-78ba-4f9d-bc24-1e0ac92218d7",
   "metadata": {},
   "source": [
    "You should consider a tool like Nvidia's [DALI](https://github.com/NVIDIA/DALI) to create pre/post processing pipelines. They can even be intergrated into [Triton](https://github.com/triton-inference-server/dali_backend) for a complete end to end inference pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c49c6-c6a3-4fdf-a71e-52cd3e4dc4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
